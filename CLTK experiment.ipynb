{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import cltk\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "\n",
    "# run this only once to set up the corpus files for lemmatizing and such\n",
    "# def setup_files():\n",
    "    # from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "    # corpus_importer = CorpusImporter('latin')\n",
    "    # corpus_importer.list_corpora\n",
    "    # corpus_importer.import_corpus('latin_text_latin_library')\n",
    "    # corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#set up tokenizer \n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "\n",
    "# set up fancy lemmatizer\n",
    "# rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "# path = os.path.expanduser(rel_path)\n",
    "# file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "# latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "\n",
    "# if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "#     latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "# else:\n",
    "#     latin_pos_lemmatized_sents = []\n",
    "#     print('The file %s is not available in cltk_data' % file)\n",
    "\n",
    "# lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)\n",
    "\n",
    "#or just use the default\n",
    "default_lemmatizer = LemmaReplacer('latin')\n",
    "\n",
    "# test lemmatizer if you want\n",
    "# default_lemmatizer.lemmatize('amabo')\n",
    "# default_lemmatizer.lemmatize('pedis')\n",
    "\n",
    "# Remove punctuation with translate function\n",
    "punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "\n",
    "def process_text(data):\n",
    "    print('starting process')\n",
    "    works = data['works']\n",
    "    lemma_list = []\n",
    "    # make list of lines\n",
    "    for work in works:\n",
    "        books = work['books']\n",
    "        for book in books:\n",
    "            poems = book ['poems']\n",
    "            for poem in poems:\n",
    "                lines = poem['lines']\n",
    "                for line in lines:\n",
    "                    raw_line = line['text']\n",
    "                    try:\n",
    "                        text = raw_line.translate(translator)\n",
    "                        word_list = word_tokenizer.tokenize(text)\n",
    "                        for word in word_list:\n",
    "                            word = default_lemmatizer.lemmatize(word)\n",
    "#                             word = lemmatizer.lemmatize(word)\n",
    "                            print(word[0])\n",
    "                            lemma_list.append(word[0])\n",
    "                    except:\n",
    "                        pass\n",
    "    print('done lemmatizing')\n",
    "\n",
    "    # Build counter for lemma list\n",
    "    text_lemmas_counter = Counter(lemma_list)\n",
    "    text_lemmas_mc = text_lemmas_counter.most_common(100)\n",
    "\n",
    "    running = 0\n",
    "\n",
    "    print('Top 25 lemmas in Ars 1:\\n')\n",
    "    print(\"{number:>5}  {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=\"\", lemma=\"lemma\", count=\"COUNT\", percent=\"Type-Tok %\", running = \"RUNNING %\"))\n",
    "    for i, pair in enumerate(text_lemmas_mc[:50]):\n",
    "        running += pair[1]\n",
    "        print(\"{number:>5}. {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=i+1, lemma=pair[0], count=pair[1], percent=str(round(pair[1] / len(lemma_list)*100, 2))+\"%\", running = str(round(running / len(lemma_list)*100, 2))+\"%\"))\n",
    "    \n",
    "    #in case anyone needs it I guess\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "    print(lemma_text)\n",
    "\n",
    "\n",
    "#try this on the ars\n",
    "with open('ars.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "process_text(data)\n",
    "        \n",
    "\n",
    "# Refs:\n",
    "# https://github.com/diyclassics/ll-experiments/blob/master/Exploring%20Diction%20and%20Topics%20in%20Latin%20Love%20Elegy.ipynb\n",
    "# https://github.com/kynan/nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
