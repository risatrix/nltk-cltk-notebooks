{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import cltk\n",
    "\n",
    "# run this only once to set up the corpus files for lemmatizing and such\n",
    "# def setup_files():\n",
    "    # from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "    # corpus_importer = CorpusImporter('latin')\n",
    "    # corpus_importer.list_corpora\n",
    "    # corpus_importer.import_corpus('latin_text_latin_library')\n",
    "    # corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "\n",
    "\n",
    "import os\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from collections import Counter\n",
    "\n",
    "with open('corpus.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "works = data['works']\n",
    "\n",
    "line_list =[]\n",
    "word_list = []\n",
    "#set up tokenizer \n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "\n",
    "# set up fancy lemmatizer\n",
    "# rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "# path = os.path.expanduser(rel_path)\n",
    "\n",
    "# # Check for presence of latin_pos_lemmatized_sents\n",
    "# file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "\n",
    "# latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "\n",
    "# if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "#     latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "# else:\n",
    "#     latin_pos_lemmatized_sents = []\n",
    "#     print('The file %s is not available in cltk_data' % file)\n",
    "\n",
    "# lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)\n",
    "\n",
    "#or just use default\n",
    "default_lemmatizer = LemmaReplacer('latin')\n",
    "\n",
    "# def create_lemmatized_texts(texts):\n",
    "#     textin = \" ||| \".join(texts)\n",
    "#     tokens = textin.split()\n",
    "#     lemmas = lemmatizer.lemmatize(tokens)\n",
    "#     textout = \" \".join(lemma[1] for lemma in lemmas)\n",
    "#     punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{}~.?!«»—\"\n",
    "#     textout = re.sub(r' punc ', ' ', textout)\n",
    "#     lemmatized_texts = textout.split('|||')\n",
    "#     return lemmatized_texts\n",
    "\n",
    "\n",
    "# Remove punctuation with translate\n",
    "punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "\n",
    "# make list of lines\n",
    "for work in works:\n",
    "    books = work['books']\n",
    "    for book in books:\n",
    "        poems = book ['poems']\n",
    "        for poem in poems:\n",
    "            lines = poem['lines']\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    raw_line = line['text']\n",
    "                    text = raw_line.translate(translator)\n",
    "                    word_list = word_tokenizer.tokenize(text)\n",
    "                    for word in word_list:\n",
    "                        word = default_lemmatizer.lemmatize(word)\n",
    "                        word_list.append(word[0])\n",
    "                        print('word')\n",
    "#                         print(word[0])\n",
    "                except:\n",
    "                    pass\n",
    "print('done')\n",
    "print(word_list)\n",
    "# lemmas_counter = Counter(word_list)\n",
    "# lemmas_mc = lemmas_counter.most_common(100)\n",
    "\n",
    "# print (lemmas_mc)\n",
    "\n",
    "#make sublist to test lemmatizer which is very slow\n",
    "# small_list = line_list[1:3]\n",
    "# print (small_list)\n",
    "\n",
    "# for item in small_list:\n",
    "#     for word in item:\n",
    "#         word = lemmatizer.lemmatize(word)\n",
    "#     print(item)\n",
    "\n",
    "# print (small_list)\n",
    "# lemmatizer.lemmatize('amabo')\n",
    "# default_lemmatizer.lemmatize('amabo')\n",
    "# default_lemmatizer.lemmatize('pedis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
